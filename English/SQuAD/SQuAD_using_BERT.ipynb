{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQuAD using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6615cc27a1ec473da64f150e2a117f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1bb47bad854240efbb4288c15368dfb6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_313b7e6156ee423eaa1291b61ed2e7ae",
              "IPY_MODEL_5f8d4192fa6a49c5a930067090b70777"
            ]
          }
        },
        "1bb47bad854240efbb4288c15368dfb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "313b7e6156ee423eaa1291b61ed2e7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_91f7f43cbfab4366b0b6b9f072f9cf2f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbf225b1535643869fd4849e3ab27f40"
          }
        },
        "5f8d4192fa6a49c5a930067090b70777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac0d8123807f4bf093bdfa931974b40d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 3.13MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4a97680146c458d9e82b08f73c6f002"
          }
        },
        "91f7f43cbfab4366b0b6b9f072f9cf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbf225b1535643869fd4849e3ab27f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac0d8123807f4bf093bdfa931974b40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4a97680146c458d9e82b08f73c6f002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "053ce69d0e74487985ddc6f23538966b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_670d1ff53f634ef693f8aecf5b298511",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f58cab06e8b4a44988b3c574ddcd3ce",
              "IPY_MODEL_a8e66348275044bb9d1e977327d1085b"
            ]
          }
        },
        "670d1ff53f634ef693f8aecf5b298511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f58cab06e8b4a44988b3c574ddcd3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8f54f43ed24f477984bb1b9c7a727e7c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ab0a3eceaf04b6ca19c2584c0627804"
          }
        },
        "a8e66348275044bb9d1e977327d1085b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9fab693375e432ba7f1e43b9ba2d5e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:24&lt;00:00, 17.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f549ee7a83694883a32ae3fd8089858e"
          }
        },
        "8f54f43ed24f477984bb1b9c7a727e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ab0a3eceaf04b6ca19c2584c0627804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9fab693375e432ba7f1e43b9ba2d5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f549ee7a83694883a32ae3fd8089858e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d53adc25433a478bb4bff473148cfd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e1d0634dc694613ab538f4033523454",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da40f7b71faa4d63a2b7dfbe97a0311d",
              "IPY_MODEL_cecbd044104e424da49ea7ab8a618253"
            ]
          }
        },
        "3e1d0634dc694613ab538f4033523454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da40f7b71faa4d63a2b7dfbe97a0311d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f8a6108876e84e58bb1ae62ae9b8eaab",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c56517f5ffc44a0a764d226ddc92105"
          }
        },
        "cecbd044104e424da49ea7ab8a618253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dda13de45d1742c78a70118128b94c2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:05&lt;00:00, 75.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5938580b7aaa411395edc8356ceaf49f"
          }
        },
        "f8a6108876e84e58bb1ae62ae9b8eaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c56517f5ffc44a0a764d226ddc92105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dda13de45d1742c78a70118128b94c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5938580b7aaa411395edc8356ceaf49f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbdNW-qiRIz4"
      },
      "source": [
        "## **0. Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCuALLvYYSLU",
        "outputId": "e0c149be-589d-4464-f0e3-097e3b880eea"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 16.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=354279d52b24b18c9314871e64db51060d822c1d4c940cdffc1142c3d40ce35a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHotb9RbQcVg"
      },
      "source": [
        "#import used library\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import os\r\n",
        "import textwrap\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW, BertConfig, get_linear_schedule_with_warmup\r\n",
        "from torch.utils.data import TensorDataset, random_split\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2tSpd28jxdT"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V37dyIGxW90d"
      },
      "source": [
        "## **1. Import the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTvuruL1RSzB"
      },
      "source": [
        "with open(os.path.join('./data/train-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\r\n",
        "    input_data = json.load(reader)[\"data\"]\r\n",
        "\r\n",
        "dataset = []\r\n",
        "\r\n",
        "for sample in input_data:\r\n",
        "    # Title of the the article\r\n",
        "    title = sample['title']\r\n",
        "    # each article is divided into many paragraphs\r\n",
        "    paragraphs = sample['paragraphs']\r\n",
        "    # for each paragraph in the article\r\n",
        "    for paragraph in paragraphs:\r\n",
        "        # text of the paragraph\r\n",
        "        context = paragraph['context']\r\n",
        "        qas = paragraph['qas']\r\n",
        "        for qa in qas:\r\n",
        "            # to save each sample\r\n",
        "            sample = {}\r\n",
        "            sample[\"title\"] = title\r\n",
        "            sample[\"context\"] = context\r\n",
        "            # the question\r\n",
        "            sample[\"question\"] = qa['question']\r\n",
        "            sample[\"id\"] = qa['id']\r\n",
        "            answer = qa['answers'][0]\r\n",
        "            sample[\"answer_text\"] = answer['text']\r\n",
        "\r\n",
        "            sample[\"answer_start\"] = answer['answer_start']\r\n",
        "            # pass the sample to the dataset\r\n",
        "            dataset.append(sample)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WpDcgZeTp9c",
        "outputId": "fc012417-b717-41f8-9a3a-8436b2978e7d"
      },
      "source": [
        "# Wrap text to 80 characters.\r\n",
        "wrapper = textwrap.TextWrapper(width=80) \r\n",
        "\r\n",
        "# Select an example to check out.\r\n",
        "ex = dataset[888]\r\n",
        "\r\n",
        "print('Title:', ex['title'])\r\n",
        "print('ID:', ex['id'])\r\n",
        "\r\n",
        "print('\\n======== Question =========')\r\n",
        "print(ex['question'])\r\n",
        "\r\n",
        "print('\\n======== Context =========')\r\n",
        "print(wrapper.fill(ex['context']))\r\n",
        "\r\n",
        "print('\\n======== Answer =========')\r\n",
        "print(ex['answer_text'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title: Beyoncé\n",
            "ID: 56becb8d3aeaaa14008c9499\n",
            "\n",
            "======== Question =========\n",
            "Who was the first female to achieve the International Artist Award at the American Music Awards?\n",
            "\n",
            "======== Context =========\n",
            "Beyoncé has received numerous awards. As a solo artist she has sold over 15\n",
            "million albums in the US, and over 118 million records worldwide (a further 60\n",
            "million additionally with Destiny's Child), making her one of the best-selling\n",
            "music artists of all time. The Recording Industry Association of America (RIAA)\n",
            "listed Beyoncé as the top certified artist of the 2000s, with a total of 64\n",
            "certifications. Her songs \"Crazy in Love\", \"Single Ladies (Put a Ring on It)\",\n",
            "\"Halo\", and \"Irreplaceable\" are some of the best-selling singles of all time\n",
            "worldwide. In 2009, The Observer named her the Artist of the Decade and\n",
            "Billboard named her the Top Female Artist and Top Radio Songs Artist of the\n",
            "Decade. In 2010, Billboard named her in their \"Top 50 R&B/Hip-Hop Artists of the\n",
            "Past 25 Years\" list at number 15. In 2012 VH1 ranked her third on their list of\n",
            "the \"100 Greatest Women in Music\". Beyoncé was the first female artist to be\n",
            "honored with the International Artist Award at the American Music Awards. She\n",
            "has also received the Legend Award at the 2008 World Music Awards and the\n",
            "Billboard Millennium Award at the 2011 Billboard Music Awards.\n",
            "\n",
            "======== Answer =========\n",
            "Beyoncé\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BMGEzlnYOQV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "6615cc27a1ec473da64f150e2a117f16",
            "1bb47bad854240efbb4288c15368dfb6",
            "313b7e6156ee423eaa1291b61ed2e7ae",
            "5f8d4192fa6a49c5a930067090b70777",
            "91f7f43cbfab4366b0b6b9f072f9cf2f",
            "bbf225b1535643869fd4849e3ab27f40",
            "ac0d8123807f4bf093bdfa931974b40d",
            "c4a97680146c458d9e82b08f73c6f002"
          ]
        },
        "id": "rJaFhc3wVaQX",
        "outputId": "2d858968-7c0f-4156-b80f-56587efa2300"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# Load the tokenizer for our model.\r\n",
        "tokenizer = BertTokenizer.from_pretrained(\r\n",
        "    'bert-base-cased',\r\n",
        "    do_lower_case=False\r\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6615cc27a1ec473da64f150e2a117f16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj0Jp1iua1h1",
        "outputId": "e484f20f-f3ae-4a0b-dc44-6db7075294fd"
      },
      "source": [
        "for elem in dataset[0]:\r\n",
        "  print(elem)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title\n",
            "context\n",
            "question\n",
            "id\n",
            "answer_text\n",
            "answer_start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcdNt0yqdz6V"
      },
      "source": [
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVUwWmk0YQeY",
        "outputId": "607ae7c6-100b-4110-9899-a493b398d0a7"
      },
      "source": [
        "# Track the time. Tokenizing all training examples takes around 3 minutes.\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "all_input_ids = []\r\n",
        "attention_masks = []\r\n",
        "segment_ids = [] \r\n",
        "start_positions = []\r\n",
        "end_positions = []\r\n",
        "\r\n",
        "num_dropped = 0\r\n",
        "\r\n",
        "update_interval = 10000\r\n",
        "\r\n",
        "print('Tokenizing {:,} examples...'.format(len(dataset)))\r\n",
        "\r\n",
        "for (ex_num, ex) in enumerate(dataset):\r\n",
        "\r\n",
        "    if (ex_num % update_interval) == 0 and not (ex_num == 0):\r\n",
        "\r\n",
        "        elapsed = format_time(time.time() - t0)\r\n",
        "        \r\n",
        "        ex_per_sec = (time.time() - t0) / ex_num\r\n",
        "        remaining_sec = ex_per_sec * (len(dataset) - ex_num)\r\n",
        "        remaining = format_time(remaining_sec)\r\n",
        "\r\n",
        "        # Report progress.\r\n",
        "        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(dataset), elapsed, remaining))\r\n",
        "\r\n",
        "\r\n",
        "    # Tokenize the answer--it may be broken into multiple words and/or subwords.\r\n",
        "    answer_tokens = tokenizer.tokenize(ex['answer_text'])\r\n",
        "\r\n",
        "    # Create our sentinel string, e.g., \"[MASK] [MASK] [MASK]\"\r\n",
        "    sentinel_str = ' '.join(['[MASK]']*len(answer_tokens))\r\n",
        "\r\n",
        "    start_char_i = ex['answer_start']\r\n",
        "    end_char_i = start_char_i + len(ex['answer_text'])\r\n",
        "\r\n",
        "    # To make the replacement, we use slicing and string concatenation.\r\n",
        "    context_w_sentinel = ex['context'][:start_char_i] + \\\r\n",
        "                         sentinel_str + \\\r\n",
        "                         ex['context'][end_char_i:]\r\n",
        "\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "        ex['question'], \r\n",
        "        context_w_sentinel,\r\n",
        "        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\r\n",
        "        max_length = 384,       # Pad & truncate all sentences.\r\n",
        "        pad_to_max_length = True,\r\n",
        "        truncation = True,\r\n",
        "        return_attention_mask = True, # Construct attention masks.\r\n",
        "        return_tensors = 'pt',        # Return pytorch tensors.\r\n",
        "    )\r\n",
        "\r\n",
        "    # Retrieve the encoded sequence.\r\n",
        "    input_ids = encoded_dict['input_ids']\r\n",
        "\r\n",
        "    # =============================\r\n",
        "    #     Locate Answer Tokens\r\n",
        "    # =============================\r\n",
        "\r\n",
        "    # First, compare all of the tokens to the mask token. \r\n",
        "    is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\r\n",
        "\r\n",
        "    # Then get the indeces of the '1's using the `nonzero` function.\r\n",
        "    mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\r\n",
        "\r\n",
        "    # Make sure the number of MASK tokens we found is the same as the number of\r\n",
        "    # answer tokens. If not, we presumably lost the answer due to truncation.\r\n",
        "    if not len(mask_token_indeces) == len(answer_tokens):\r\n",
        "        \r\n",
        "        # Tally the number of training samples that we skip due to this issue.\r\n",
        "        num_dropped += 1\r\n",
        "\r\n",
        "        continue\r\n",
        "\r\n",
        "\r\n",
        "    start_index = mask_token_indeces[0]\r\n",
        "    end_index = mask_token_indeces[-1]\r\n",
        "\r\n",
        "    # =============================\r\n",
        "    #     Restore Answer Tokens\r\n",
        "    # =============================\r\n",
        "\r\n",
        "    # Encode the answer tokens (to token ids).\r\n",
        "    answer_token_ids = tokenizer.encode(answer_tokens, \r\n",
        "                                        add_special_tokens=False, \r\n",
        "                                        return_tensors='pt')\r\n",
        "\r\n",
        "    # Restore the answer within the reference text. (Replace the `[MASK]` tokens\r\n",
        "    # with the answer tokens)\r\n",
        "    input_ids[0, start_index : end_index + 1] = answer_token_ids\r\n",
        "\r\n",
        "    # =============================\r\n",
        "    #     Store Encoded Sample\r\n",
        "    # =============================\r\n",
        "\r\n",
        "    # Add the encoded sentence to the list.    \r\n",
        "    all_input_ids.append(input_ids)\r\n",
        "\r\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])    \r\n",
        "\r\n",
        "    # Store the segment IDs, which indicate which tokens belong to the question\r\n",
        "    # vs. the context.\r\n",
        "    segment_ids.append(encoded_dict['token_type_ids'])\r\n",
        "\r\n",
        "    # Store the start and end indeces of the correct answer.\r\n",
        "    start_positions.append(start_index)\r\n",
        "    end_positions.append(end_index)\r\n",
        "\r\n",
        "\r\n",
        "# =========================\r\n",
        "#        Wrap-Up\r\n",
        "# =========================\r\n",
        "\r\n",
        "# Convert the lists of tensors into 2D tensors.\r\n",
        "all_input_ids = torch.cat(all_input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "segment_ids = torch.cat(segment_ids, dim=0)\r\n",
        "\r\n",
        "# Convert the \"labels\" (the start and end indeces) into tensors.\r\n",
        "start_positions = torch.tensor(start_positions)\r\n",
        "end_positions = torch.tensor(end_positions)\r\n",
        "\r\n",
        "print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing 87,599 examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Example  10,000  of   87,599.    Elapsed: 0:00:23. Remaining: 0:03:02\n",
            "  Example  20,000  of   87,599.    Elapsed: 0:00:43. Remaining: 0:02:26\n",
            "  Example  30,000  of   87,599.    Elapsed: 0:01:06. Remaining: 0:02:07\n",
            "  Example  40,000  of   87,599.    Elapsed: 0:01:32. Remaining: 0:01:49\n",
            "  Example  50,000  of   87,599.    Elapsed: 0:01:57. Remaining: 0:01:28\n",
            "  Example  60,000  of   87,599.    Elapsed: 0:02:23. Remaining: 0:01:06\n",
            "  Example  70,000  of   87,599.    Elapsed: 0:02:48. Remaining: 0:00:42\n",
            "  Example  80,000  of   87,599.    Elapsed: 0:03:13. Remaining: 0:00:18\n",
            "DONE.  Tokenization took 0:03:33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9W4jGfRaMHY",
        "outputId": "da6daedb-8491-4bb0-a91d-8240a37520e9"
      },
      "source": [
        "print('Tokenized {:,} examples.'.format(len(all_input_ids)))\r\n",
        "\r\n",
        "print('\\nDropped {:,} examples.'.format(num_dropped))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized 87,502 examples.\n",
            "\n",
            "Dropped 97 examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ofKuO_fiBu0",
        "outputId": "65c0d1ba-d2c0-433b-cda5-a8f94bfa39a6"
      },
      "source": [
        "# Combine the training inputs into a TensorDataset.\r\n",
        "dataset = TensorDataset(all_input_ids, \r\n",
        "                        attention_masks, \r\n",
        "                        segment_ids, \r\n",
        "                        start_positions, \r\n",
        "                        end_positions)\r\n",
        "\r\n",
        "print('Dataset size: {:} samples'.format(len(dataset)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 87502 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdbAcMRiFUf",
        "outputId": "dedbc2ba-051f-4ac7-92b3-4b395d36fdd7"
      },
      "source": [
        "# Calculate the number of samples to include in each set.\r\n",
        "train_size = int(0.98 * len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "# Divide the dataset by randomly selecting samples.\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "print('{:>5,} training samples'.format(train_size))\r\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85,751 training samples\n",
            "1,751 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNAg-nMRjSEu",
        "outputId": "b08020f6-09cf-4c9c-bfc4-25e8cf4741e8"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SubsetRandomSampler, SequentialSampler\r\n",
        "\r\n",
        "import numpy.random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "# Randomly select 10,000 indeces from the training set to use. \r\n",
        "#indeces = np.random.permutation(len(train_dataset))[:10000]\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "# We'll take training samples in random order. \r\n",
        "train_dataloader = DataLoader(\r\n",
        "            train_dataset,  # The training samples.\r\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
        "            #sampler = SubsetRandomSampler(indeces, train_dataset),\r\n",
        "            batch_size = batch_size # Trains with this batch size.\r\n",
        "        )\r\n",
        "\r\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
        "validation_dataloader = DataLoader(\r\n",
        "            val_dataset, # The validation samples.\r\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
        "            batch_size = batch_size # Evaluate with this batch size.\r\n",
        "        )\r\n",
        "\r\n",
        "print('{:,} training batches & {:,} validation batches'.format(len(train_dataloader), len(validation_dataloader)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5,360 training batches & 110 validation batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "053ce69d0e74487985ddc6f23538966b",
            "670d1ff53f634ef693f8aecf5b298511",
            "3f58cab06e8b4a44988b3c574ddcd3ce",
            "a8e66348275044bb9d1e977327d1085b",
            "8f54f43ed24f477984bb1b9c7a727e7c",
            "6ab0a3eceaf04b6ca19c2584c0627804",
            "f9fab693375e432ba7f1e43b9ba2d5e1",
            "f549ee7a83694883a32ae3fd8089858e",
            "d53adc25433a478bb4bff473148cfd7a",
            "3e1d0634dc694613ab538f4033523454",
            "da40f7b71faa4d63a2b7dfbe97a0311d",
            "cecbd044104e424da49ea7ab8a618253",
            "f8a6108876e84e58bb1ae62ae9b8eaab",
            "4c56517f5ffc44a0a764d226ddc92105",
            "dda13de45d1742c78a70118128b94c2c",
            "5938580b7aaa411395edc8356ceaf49f"
          ]
        },
        "id": "jBBti6bnghDY",
        "outputId": "e6b13ffa-f326-4dc6-ed1a-4affc2c1bd72"
      },
      "source": [
        "\r\n",
        "model = BertForQuestionAnswering.from_pretrained(\r\n",
        "    \"bert-base-cased\", \r\n",
        "    output_attentions = False, \r\n",
        "    output_hidden_states = False\r\n",
        ")\r\n",
        "\r\n",
        "desc = model.cuda()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "053ce69d0e74487985ddc6f23538966b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d53adc25433a478bb4bff473148cfd7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBrheyaRjXTK"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5\r\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                )\r\n",
        "\r\n",
        "# Number of training epochs (authors recommend between 2 and 4)\r\n",
        "epochs = 3\r\n",
        "\r\n",
        "# Total number of training steps is number of batches * number of epochs.\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwFoBVlrjjsU",
        "outputId": "fc6332d2-b487-4694-d369-5ca7172b3a6a"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# This training code is based on the `run_glue.py` script here:\r\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val = 42\r\n",
        "\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# We'll store a number of quantities such as training and validation loss, \r\n",
        "# validation accuracy, and timings.\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    \r\n",
        "    print('Training {:,} batches...'.format(len(train_dataloader)))\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be mislead--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # Pick an interval on which to print progress updates.\r\n",
        "    update_interval = 100\r\n",
        "\r\n",
        "\r\n",
        "    # The total number of batches per epoch.\r\n",
        "    num_batches = len(train_dataloader)\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every, e.g., 500 batches.\r\n",
        "        if step % update_interval == 0 and not step == 0:\r\n",
        "            \r\n",
        "            # Calculate elapsed time and format it.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Calculate the time remaining based on our progress.\r\n",
        "            step_per_sec = (time.time() - t0) / step\r\n",
        "            remaining_sec = step_per_sec * (num_batches - step)\r\n",
        "            remaining = format_time(remaining_sec)\r\n",
        "\r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(step, num_batches, elapsed, remaining))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_seg_ids = batch[2].to(device)\r\n",
        "        b_start_pos = batch[3].to(device)\r\n",
        "        b_end_pos = batch[4].to(device)\r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        # It returns different numbers of parameters depending on what arguments\r\n",
        "        # arge given and what flags are set. For our useage here, it returns\r\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\r\n",
        "        # outputs prior to activation.\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                        attention_mask=b_input_mask, \r\n",
        "                        token_type_ids = b_seg_ids,\r\n",
        "                        start_positions=b_start_pos,\r\n",
        "                        end_positions=b_end_pos)\r\n",
        "\r\n",
        "        # You can see the outputs in the source code here:\r\n",
        "        # https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1601\r\n",
        "        # \r\n",
        "        # The forward pass returns the loss, start_logits, and end_logits.\r\n",
        "        #(loss, start_logits, end_logits) = outputs\r\n",
        "        loss = outputs[0]\r\n",
        "        start_logits = outputs[1]\r\n",
        "        end_logits = outputs[2]\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss += loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    # Measure how long this epoch took.\r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    # After the completion of each training epoch, measure our performance on\r\n",
        "    # our validation set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "    # during evaluation.\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "\r\n",
        "    t0_val = time.time()\r\n",
        "\r\n",
        "    # Tracking results. \r\n",
        "    pred_start, pred_end, true_start, true_end = [], [], [], []\r\n",
        "\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
        "        # the `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_seg_ids = batch[2].to(device)\r\n",
        "        b_start_pos = batch[3].to(device)\r\n",
        "        b_end_pos = batch[4].to(device)\r\n",
        "        \r\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\r\n",
        "        # the forward pass, since this is only needed for backprop (training).\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            # Forward pass, calculate logit predictions.\r\n",
        "            # token_type_ids is the same as the \"segment ids\", which \r\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "            # values prior to applying an activation function like the softmax.\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                            token_type_ids=b_seg_ids, \r\n",
        "                            attention_mask=b_input_mask,\r\n",
        "                            start_positions=b_start_pos,\r\n",
        "                            end_positions=b_end_pos)\r\n",
        "\r\n",
        "        # You can see the outputs in the source code here:\r\n",
        "        # https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1601\r\n",
        "        # \r\n",
        "        # The forward pass returns the loss, start_logits, and end_logits.\r\n",
        "        loss = outputs[0]\r\n",
        "        start_logits = outputs[1]\r\n",
        "        end_logits = outputs[2]\r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        start_logits = start_logits.detach().cpu().numpy()\r\n",
        "        end_logits = end_logits.detach().cpu().numpy()\r\n",
        "        \r\n",
        "        # Move the correct start and end positions back to the CPU.\r\n",
        "        b_start_pos = b_start_pos.to('cpu').numpy()\r\n",
        "        b_end_pos = b_end_pos.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Find the tokens with the highest `start` and `end` scores.\r\n",
        "        answer_start = np.argmax(start_logits, axis=1)\r\n",
        "        answer_end = np.argmax(end_logits, axis=1)\r\n",
        "\r\n",
        "        # Store predictions and true labels\r\n",
        "        pred_start.append(answer_start)\r\n",
        "        pred_end.append(answer_end)\r\n",
        "        true_start.append(b_start_pos)\r\n",
        "        true_end.append(b_end_pos)\r\n",
        "\r\n",
        "    # Combine the results across the batches.\r\n",
        "    pred_start = np.concatenate(pred_start, axis=0)\r\n",
        "    pred_end = np.concatenate(pred_end, axis=0)\r\n",
        "    true_start = np.concatenate(true_start, axis=0)\r\n",
        "    true_end = np.concatenate(true_end, axis=0)\r\n",
        "        \r\n",
        "    # Count up the number of start index predictions and end index predictions \r\n",
        "    # which match the correct indeces.\r\n",
        "    num_start_correct = np.sum(pred_start == true_start)\r\n",
        "    num_end_correct = np.sum(pred_end == true_end)\r\n",
        "\r\n",
        "    total_correct = num_start_correct + num_end_correct\r\n",
        "    total_indeces = len(true_start) + len(true_end)\r\n",
        "\r\n",
        "    # Report the final accuracy for this validation run.\r\n",
        "    avg_val_accuracy = float(total_correct) / float(total_indeces)\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    \r\n",
        "    validation_time = format_time(time.time() - t0_val)\r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Record all statistics from this epoch.\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training 5,360 batches...\n",
            "  Batch     100  of    5,360.    Elapsed: 0:02:00. Remaining: 1:45:10\n",
            "  Batch     200  of    5,360.    Elapsed: 0:04:07. Remaining: 1:46:24\n",
            "  Batch     300  of    5,360.    Elapsed: 0:06:16. Remaining: 1:45:40\n",
            "  Batch     400  of    5,360.    Elapsed: 0:08:24. Remaining: 1:44:09\n",
            "  Batch     500  of    5,360.    Elapsed: 0:10:32. Remaining: 1:42:27\n",
            "  Batch     600  of    5,360.    Elapsed: 0:12:41. Remaining: 1:40:33\n",
            "  Batch     700  of    5,360.    Elapsed: 0:14:49. Remaining: 1:38:36\n",
            "  Batch     800  of    5,360.    Elapsed: 0:16:57. Remaining: 1:36:36\n",
            "  Batch     900  of    5,360.    Elapsed: 0:19:05. Remaining: 1:34:33\n",
            "  Batch   1,000  of    5,360.    Elapsed: 0:21:13. Remaining: 1:32:31\n",
            "  Batch   1,100  of    5,360.    Elapsed: 0:23:21. Remaining: 1:30:26\n",
            "  Batch   1,200  of    5,360.    Elapsed: 0:25:30. Remaining: 1:28:23\n",
            "  Batch   1,300  of    5,360.    Elapsed: 0:27:37. Remaining: 1:26:16\n",
            "  Batch   1,400  of    5,360.    Elapsed: 0:29:46. Remaining: 1:24:12\n",
            "  Batch   1,500  of    5,360.    Elapsed: 0:31:55. Remaining: 1:22:07\n",
            "  Batch   1,600  of    5,360.    Elapsed: 0:34:01. Remaining: 1:19:57\n",
            "  Batch   1,700  of    5,360.    Elapsed: 0:36:07. Remaining: 1:17:46\n",
            "  Batch   1,800  of    5,360.    Elapsed: 0:38:13. Remaining: 1:15:36\n",
            "  Batch   1,900  of    5,360.    Elapsed: 0:40:19. Remaining: 1:13:26\n",
            "  Batch   2,000  of    5,360.    Elapsed: 0:42:25. Remaining: 1:11:15\n",
            "  Batch   2,100  of    5,360.    Elapsed: 0:44:30. Remaining: 1:09:05\n",
            "  Batch   2,200  of    5,360.    Elapsed: 0:46:36. Remaining: 1:06:56\n",
            "  Batch   2,300  of    5,360.    Elapsed: 0:48:41. Remaining: 1:04:47\n",
            "  Batch   2,400  of    5,360.    Elapsed: 0:50:47. Remaining: 1:02:38\n",
            "  Batch   2,500  of    5,360.    Elapsed: 0:52:53. Remaining: 1:00:30\n",
            "  Batch   2,600  of    5,360.    Elapsed: 0:54:59. Remaining: 0:58:22\n",
            "  Batch   2,700  of    5,360.    Elapsed: 0:57:04. Remaining: 0:56:13\n",
            "  Batch   2,800  of    5,360.    Elapsed: 0:59:10. Remaining: 0:54:05\n",
            "  Batch   2,900  of    5,360.    Elapsed: 1:01:15. Remaining: 0:51:58\n",
            "  Batch   3,000  of    5,360.    Elapsed: 1:03:21. Remaining: 0:49:50\n",
            "  Batch   3,100  of    5,360.    Elapsed: 1:05:27. Remaining: 0:47:43\n",
            "  Batch   3,200  of    5,360.    Elapsed: 1:07:32. Remaining: 0:45:35\n",
            "  Batch   3,300  of    5,360.    Elapsed: 1:09:38. Remaining: 0:43:28\n",
            "  Batch   3,400  of    5,360.    Elapsed: 1:11:44. Remaining: 0:41:21\n",
            "  Batch   3,500  of    5,360.    Elapsed: 1:13:49. Remaining: 0:39:14\n",
            "  Batch   3,600  of    5,360.    Elapsed: 1:15:55. Remaining: 0:37:07\n",
            "  Batch   3,700  of    5,360.    Elapsed: 1:18:01. Remaining: 0:35:00\n",
            "  Batch   3,800  of    5,360.    Elapsed: 1:20:09. Remaining: 0:32:54\n",
            "  Batch   3,900  of    5,360.    Elapsed: 1:22:17. Remaining: 0:30:48\n",
            "  Batch   4,000  of    5,360.    Elapsed: 1:24:25. Remaining: 0:28:42\n",
            "  Batch   4,100  of    5,360.    Elapsed: 1:26:34. Remaining: 0:26:36\n",
            "  Batch   4,200  of    5,360.    Elapsed: 1:28:42. Remaining: 0:24:30\n",
            "  Batch   4,300  of    5,360.    Elapsed: 1:30:50. Remaining: 0:22:24\n",
            "  Batch   4,400  of    5,360.    Elapsed: 1:32:58. Remaining: 0:20:17\n",
            "  Batch   4,500  of    5,360.    Elapsed: 1:35:06. Remaining: 0:18:10\n",
            "  Batch   4,600  of    5,360.    Elapsed: 1:37:14. Remaining: 0:16:04\n",
            "  Batch   4,700  of    5,360.    Elapsed: 1:39:22. Remaining: 0:13:57\n",
            "  Batch   4,800  of    5,360.    Elapsed: 1:41:30. Remaining: 0:11:50\n",
            "  Batch   4,900  of    5,360.    Elapsed: 1:43:38. Remaining: 0:09:44\n",
            "  Batch   5,000  of    5,360.    Elapsed: 1:45:46. Remaining: 0:07:37\n",
            "  Batch   5,100  of    5,360.    Elapsed: 1:47:55. Remaining: 0:05:30\n",
            "  Batch   5,200  of    5,360.    Elapsed: 1:50:03. Remaining: 0:03:23\n",
            "  Batch   5,300  of    5,360.    Elapsed: 1:52:11. Remaining: 0:01:16\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epcoh took: 1:53:27\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 1.06\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training 5,360 batches...\n",
            "  Batch     100  of    5,360.    Elapsed: 0:02:08. Remaining: 1:52:11\n",
            "  Batch     200  of    5,360.    Elapsed: 0:04:16. Remaining: 1:50:14\n",
            "  Batch     300  of    5,360.    Elapsed: 0:06:24. Remaining: 1:48:04\n",
            "  Batch     400  of    5,360.    Elapsed: 0:08:33. Remaining: 1:45:57\n",
            "  Batch     500  of    5,360.    Elapsed: 0:10:41. Remaining: 1:43:49\n",
            "  Batch     600  of    5,360.    Elapsed: 0:12:49. Remaining: 1:41:40\n",
            "  Batch     700  of    5,360.    Elapsed: 0:14:57. Remaining: 1:39:32\n",
            "  Batch     800  of    5,360.    Elapsed: 0:17:05. Remaining: 1:37:23\n",
            "  Batch     900  of    5,360.    Elapsed: 0:19:13. Remaining: 1:35:14\n",
            "  Batch   1,000  of    5,360.    Elapsed: 0:21:21. Remaining: 1:33:06\n",
            "  Batch   1,100  of    5,360.    Elapsed: 0:23:29. Remaining: 1:30:58\n",
            "  Batch   1,200  of    5,360.    Elapsed: 0:25:37. Remaining: 1:28:50\n",
            "  Batch   1,300  of    5,360.    Elapsed: 0:27:46. Remaining: 1:26:42\n",
            "  Batch   1,400  of    5,360.    Elapsed: 0:29:54. Remaining: 1:24:35\n",
            "  Batch   1,500  of    5,360.    Elapsed: 0:32:02. Remaining: 1:22:27\n",
            "  Batch   1,600  of    5,360.    Elapsed: 0:34:11. Remaining: 1:20:19\n",
            "  Batch   1,700  of    5,360.    Elapsed: 0:36:19. Remaining: 1:18:11\n",
            "  Batch   1,800  of    5,360.    Elapsed: 0:38:27. Remaining: 1:16:03\n",
            "  Batch   1,900  of    5,360.    Elapsed: 0:40:35. Remaining: 1:13:55\n",
            "  Batch   2,000  of    5,360.    Elapsed: 0:42:43. Remaining: 1:11:46\n",
            "  Batch   2,100  of    5,360.    Elapsed: 0:44:51. Remaining: 1:09:37\n",
            "  Batch   2,200  of    5,360.    Elapsed: 0:46:59. Remaining: 1:07:29\n",
            "  Batch   2,300  of    5,360.    Elapsed: 0:49:07. Remaining: 1:05:21\n",
            "  Batch   2,400  of    5,360.    Elapsed: 0:51:16. Remaining: 1:03:13\n",
            "  Batch   2,500  of    5,360.    Elapsed: 0:53:24. Remaining: 1:01:05\n",
            "  Batch   2,600  of    5,360.    Elapsed: 0:55:31. Remaining: 0:58:56\n",
            "  Batch   2,700  of    5,360.    Elapsed: 0:57:39. Remaining: 0:56:48\n",
            "  Batch   2,800  of    5,360.    Elapsed: 0:59:48. Remaining: 0:54:40\n",
            "  Batch   2,900  of    5,360.    Elapsed: 1:01:56. Remaining: 0:52:32\n",
            "  Batch   3,000  of    5,360.    Elapsed: 1:04:04. Remaining: 0:50:24\n",
            "  Batch   3,100  of    5,360.    Elapsed: 1:06:12. Remaining: 0:48:16\n",
            "  Batch   3,200  of    5,360.    Elapsed: 1:08:20. Remaining: 0:46:08\n",
            "  Batch   3,300  of    5,360.    Elapsed: 1:10:28. Remaining: 0:43:59\n",
            "  Batch   3,400  of    5,360.    Elapsed: 1:12:36. Remaining: 0:41:51\n",
            "  Batch   3,500  of    5,360.    Elapsed: 1:14:44. Remaining: 0:39:43\n",
            "  Batch   3,600  of    5,360.    Elapsed: 1:16:52. Remaining: 0:37:35\n",
            "  Batch   3,700  of    5,360.    Elapsed: 1:19:00. Remaining: 0:35:26\n",
            "  Batch   3,800  of    5,360.    Elapsed: 1:21:08. Remaining: 0:33:18\n",
            "  Batch   3,900  of    5,360.    Elapsed: 1:23:16. Remaining: 0:31:10\n",
            "  Batch   4,000  of    5,360.    Elapsed: 1:25:23. Remaining: 0:29:02\n",
            "  Batch   4,100  of    5,360.    Elapsed: 1:27:31. Remaining: 0:26:54\n",
            "  Batch   4,200  of    5,360.    Elapsed: 1:29:40. Remaining: 0:24:46\n",
            "  Batch   4,300  of    5,360.    Elapsed: 1:31:48. Remaining: 0:22:38\n",
            "  Batch   4,400  of    5,360.    Elapsed: 1:33:55. Remaining: 0:20:30\n",
            "  Batch   4,500  of    5,360.    Elapsed: 1:36:04. Remaining: 0:18:21\n",
            "  Batch   4,600  of    5,360.    Elapsed: 1:38:12. Remaining: 0:16:13\n",
            "  Batch   4,700  of    5,360.    Elapsed: 1:40:20. Remaining: 0:14:05\n",
            "  Batch   4,800  of    5,360.    Elapsed: 1:42:28. Remaining: 0:11:57\n",
            "  Batch   4,900  of    5,360.    Elapsed: 1:44:36. Remaining: 0:09:49\n",
            "  Batch   5,000  of    5,360.    Elapsed: 1:46:44. Remaining: 0:07:41\n",
            "  Batch   5,100  of    5,360.    Elapsed: 1:48:52. Remaining: 0:05:33\n",
            "  Batch   5,200  of    5,360.    Elapsed: 1:51:00. Remaining: 0:03:25\n",
            "  Batch   5,300  of    5,360.    Elapsed: 1:53:09. Remaining: 0:01:17\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epcoh took: 1:54:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 1.01\n",
            "  Validation took: 0:00:49\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training 5,360 batches...\n",
            "  Batch     100  of    5,360.    Elapsed: 0:02:08. Remaining: 1:52:07\n",
            "  Batch     200  of    5,360.    Elapsed: 0:04:16. Remaining: 1:50:02\n",
            "  Batch     300  of    5,360.    Elapsed: 0:06:24. Remaining: 1:47:56\n",
            "  Batch     400  of    5,360.    Elapsed: 0:08:32. Remaining: 1:45:53\n",
            "  Batch     500  of    5,360.    Elapsed: 0:10:40. Remaining: 1:43:43\n",
            "  Batch     600  of    5,360.    Elapsed: 0:12:49. Remaining: 1:41:38\n",
            "  Batch     700  of    5,360.    Elapsed: 0:14:57. Remaining: 1:39:29\n",
            "  Batch     800  of    5,360.    Elapsed: 0:17:05. Remaining: 1:37:20\n",
            "  Batch     900  of    5,360.    Elapsed: 0:19:13. Remaining: 1:35:11\n",
            "  Batch   1,000  of    5,360.    Elapsed: 0:21:21. Remaining: 1:33:04\n",
            "  Batch   1,100  of    5,360.    Elapsed: 0:23:29. Remaining: 1:30:55\n",
            "  Batch   1,200  of    5,360.    Elapsed: 0:25:36. Remaining: 1:28:46\n",
            "  Batch   1,300  of    5,360.    Elapsed: 0:27:44. Remaining: 1:26:38\n",
            "  Batch   1,400  of    5,360.    Elapsed: 0:29:52. Remaining: 1:24:29\n",
            "  Batch   1,500  of    5,360.    Elapsed: 0:32:01. Remaining: 1:22:22\n",
            "  Batch   1,600  of    5,360.    Elapsed: 0:34:09. Remaining: 1:20:14\n",
            "  Batch   1,700  of    5,360.    Elapsed: 0:36:17. Remaining: 1:18:06\n",
            "  Batch   1,800  of    5,360.    Elapsed: 0:38:25. Remaining: 1:15:58\n",
            "  Batch   1,900  of    5,360.    Elapsed: 0:40:33. Remaining: 1:13:51\n",
            "  Batch   2,000  of    5,360.    Elapsed: 0:42:41. Remaining: 1:11:42\n",
            "  Batch   2,100  of    5,360.    Elapsed: 0:44:49. Remaining: 1:09:35\n",
            "  Batch   2,200  of    5,360.    Elapsed: 0:46:57. Remaining: 1:07:27\n",
            "  Batch   2,300  of    5,360.    Elapsed: 0:49:06. Remaining: 1:05:19\n",
            "  Batch   2,400  of    5,360.    Elapsed: 0:51:13. Remaining: 1:03:10\n",
            "  Batch   2,500  of    5,360.    Elapsed: 0:53:21. Remaining: 1:01:02\n",
            "  Batch   2,600  of    5,360.    Elapsed: 0:55:29. Remaining: 0:58:54\n",
            "  Batch   2,700  of    5,360.    Elapsed: 0:57:37. Remaining: 0:56:46\n",
            "  Batch   2,800  of    5,360.    Elapsed: 0:59:45. Remaining: 0:54:38\n",
            "  Batch   2,900  of    5,360.    Elapsed: 1:01:53. Remaining: 0:52:30\n",
            "  Batch   3,000  of    5,360.    Elapsed: 1:04:01. Remaining: 0:50:22\n",
            "  Batch   3,100  of    5,360.    Elapsed: 1:06:09. Remaining: 0:48:14\n",
            "  Batch   3,200  of    5,360.    Elapsed: 1:08:17. Remaining: 0:46:06\n",
            "  Batch   3,300  of    5,360.    Elapsed: 1:10:25. Remaining: 0:43:58\n",
            "  Batch   3,400  of    5,360.    Elapsed: 1:12:33. Remaining: 0:41:49\n",
            "  Batch   3,500  of    5,360.    Elapsed: 1:14:41. Remaining: 0:39:41\n",
            "  Batch   3,600  of    5,360.    Elapsed: 1:16:49. Remaining: 0:37:33\n",
            "  Batch   3,700  of    5,360.    Elapsed: 1:18:57. Remaining: 0:35:25\n",
            "  Batch   3,800  of    5,360.    Elapsed: 1:21:06. Remaining: 0:33:18\n",
            "  Batch   3,900  of    5,360.    Elapsed: 1:23:14. Remaining: 0:31:09\n",
            "  Batch   4,000  of    5,360.    Elapsed: 1:25:22. Remaining: 0:29:01\n",
            "  Batch   4,100  of    5,360.    Elapsed: 1:27:29. Remaining: 0:26:53\n",
            "  Batch   4,200  of    5,360.    Elapsed: 1:29:37. Remaining: 0:24:45\n",
            "  Batch   4,300  of    5,360.    Elapsed: 1:31:45. Remaining: 0:22:37\n",
            "  Batch   4,400  of    5,360.    Elapsed: 1:33:53. Remaining: 0:20:29\n",
            "  Batch   4,500  of    5,360.    Elapsed: 1:36:01. Remaining: 0:18:21\n",
            "  Batch   4,600  of    5,360.    Elapsed: 1:38:09. Remaining: 0:16:13\n",
            "  Batch   4,700  of    5,360.    Elapsed: 1:40:17. Remaining: 0:14:05\n",
            "  Batch   4,800  of    5,360.    Elapsed: 1:42:25. Remaining: 0:11:57\n",
            "  Batch   4,900  of    5,360.    Elapsed: 1:44:33. Remaining: 0:09:49\n",
            "  Batch   5,000  of    5,360.    Elapsed: 1:46:41. Remaining: 0:07:41\n",
            "  Batch   5,100  of    5,360.    Elapsed: 1:48:49. Remaining: 0:05:33\n",
            "  Batch   5,200  of    5,360.    Elapsed: 1:50:57. Remaining: 0:03:25\n",
            "  Batch   5,300  of    5,360.    Elapsed: 1:53:05. Remaining: 0:01:17\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 1:54:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:48\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bfhY8h-jqKE",
        "outputId": "f5a66b1e-98d4-4263-a2f6-e743d16ed356"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "# Open the training dataset file.\r\n",
        "with open(os.path.join('./data/dev-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\r\n",
        "    input_data = json.load(reader)[\"data\"]\r\n",
        "\r\n",
        "\r\n",
        "print_count = 0\r\n",
        "\r\n",
        "print('Unpacking SQuAD Examples...')\r\n",
        "\r\n",
        "print('Articles:')\r\n",
        "\r\n",
        "# We'll unpack all of the \r\n",
        "examples = []\r\n",
        "\r\n",
        "# For each Wikipedia article in the dataset...\r\n",
        "for entry in input_data:\r\n",
        "\r\n",
        "    # The Wikipedia Article title.\r\n",
        "    title = entry[\"title\"]\r\n",
        "    print('  ', title)\r\n",
        "\r\n",
        "    # The article contains multiple paragraphs...\r\n",
        "    for paragraph in entry[\"paragraphs\"]:\r\n",
        "        \r\n",
        "        # The paragraph, where the answer is found, is referred to as the\r\n",
        "        # \"context\".\r\n",
        "        context_text = paragraph[\"context\"]\r\n",
        "        \r\n",
        "        # There can be multiple questions per paragraph.\r\n",
        "        for qa in paragraph[\"qas\"]:\r\n",
        "            \r\n",
        "            # Define a dictionary to store the properties.\r\n",
        "            ex = {}\r\n",
        "\r\n",
        "            # The unique ID of this question.\r\n",
        "            ex['qas_id'] = qa[\"id\"]\r\n",
        "\r\n",
        "            # The question.\r\n",
        "            ex['question_text'] = qa[\"question\"]\r\n",
        "\r\n",
        "            # In the test data, there are three answers per question, so we'll\r\n",
        "            # store all three. \r\n",
        "            # Each answer has two fields: `answer_start` and `text`.\r\n",
        "            ex['answers'] = qa[\"answers\"]\r\n",
        "\r\n",
        "            # Store the title and paragraph text.\r\n",
        "            ex['title'] = title\r\n",
        "            ex['context_text'] = context_text\r\n",
        "\r\n",
        "            examples.append(ex)\r\n",
        "\r\n",
        "print('DONE!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unpacking SQuAD Examples...\n",
            "Articles:\n",
            "   Super_Bowl_50\n",
            "   Warsaw\n",
            "   Normans\n",
            "   Nikola_Tesla\n",
            "   Computational_complexity_theory\n",
            "   Teacher\n",
            "   Martin_Luther\n",
            "   Southern_California\n",
            "   Sky_(United_Kingdom)\n",
            "   Victoria_(Australia)\n",
            "   Huguenot\n",
            "   Steam_engine\n",
            "   Oxygen\n",
            "   1973_oil_crisis\n",
            "   Apollo_program\n",
            "   European_Union_law\n",
            "   Amazon_rainforest\n",
            "   Ctenophora\n",
            "   Fresno,_California\n",
            "   Packet_switching\n",
            "   Black_Death\n",
            "   Geology\n",
            "   Newcastle_upon_Tyne\n",
            "   Victoria_and_Albert_Museum\n",
            "   American_Broadcasting_Company\n",
            "   Genghis_Khan\n",
            "   Pharmacy\n",
            "   Immune_system\n",
            "   Civil_disobedience\n",
            "   Construction\n",
            "   Private_school\n",
            "   Harvard_University\n",
            "   Jacksonville,_Florida\n",
            "   Economic_inequality\n",
            "   Doctor_Who\n",
            "   University_of_Chicago\n",
            "   Yuan_dynasty\n",
            "   Kenya\n",
            "   Intergovernmental_Panel_on_Climate_Change\n",
            "   Chloroplast\n",
            "   Prime_number\n",
            "   Rhine\n",
            "   Scottish_Parliament\n",
            "   Islamism\n",
            "   Imperialism\n",
            "   United_Methodist_Church\n",
            "   French_and_Indian_War\n",
            "   Force\n",
            "DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5DoKZnRNV4k"
      },
      "source": [
        "def good_update_interval(total_iters, num_desired_updates):\r\n",
        "    '''\r\n",
        "    This function will try to pick an intelligent progress update interval \r\n",
        "    based on the magnitude of the total iterations.\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "      `total_iters` - The number of iterations in the for-loop.\r\n",
        "      `num_desired_updates` - How many times we want to see an update over the \r\n",
        "                              course of the for-loop.\r\n",
        "    '''\r\n",
        "    # Divide the total iterations by the desired number of updates. Most likely\r\n",
        "    # this will be some ugly number.\r\n",
        "    exact_interval = total_iters / num_desired_updates\r\n",
        "\r\n",
        "    # The `round` function has the ability to round down a number to, e.g., the\r\n",
        "    # nearest thousandth: round(exact_interval, -3)\r\n",
        "    #\r\n",
        "    # To determine the magnitude to round to, find the magnitude of the total,\r\n",
        "    # and then go one magnitude below that.\r\n",
        "\r\n",
        "    # Get the order of magnitude of the total.\r\n",
        "    order_of_mag = len(str(total_iters)) - 1\r\n",
        "\r\n",
        "    # Our update interval should be rounded to an order of magnitude smaller. \r\n",
        "    round_mag = order_of_mag - 1\r\n",
        "\r\n",
        "    # Round down and cast to an int.\r\n",
        "    update_interval = int(round(exact_interval, -round_mag))\r\n",
        "\r\n",
        "    # Don't allow the interval to be zero!\r\n",
        "    if update_interval == 0:\r\n",
        "        update_interval = 1\r\n",
        "\r\n",
        "    return update_interval"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTBakV2MAcR7",
        "outputId": "4dcf506d-d6ed-4e4a-f91c-127cda612029"
      },
      "source": [
        "import time\r\n",
        "import torch\r\n",
        "\r\n",
        "import logging\r\n",
        "\r\n",
        "# By default, the tokenizer will spit out a warning whenever we tokenize a \r\n",
        "# sample which ends up being more than 512 tokens. We don't care about that for\r\n",
        "# now, though, and this cell will produce a lot of those warnings! So we'll \r\n",
        "# adjust the logging settings to suppress those warnings and keep the output\r\n",
        "# cell cleaner.\r\n",
        "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\r\n",
        "\r\n",
        "# Track the time. Tokenizing all training examples takes around 3 minutes.\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "# Lists to store locations\r\n",
        "start_positions = []\r\n",
        "end_positions = []\r\n",
        "\r\n",
        "# We'll count up the number of answers which are truncated, as well as the\r\n",
        "# number of test samples for which all three answers were truncated (it's \r\n",
        "# impossible for us to answer these).\r\n",
        "num_clipped_answers = 0\r\n",
        "num_impossible = 0\r\n",
        "\r\n",
        "# Pick an interval on which to print progress updates.\r\n",
        "update_interval = good_update_interval(\r\n",
        "            total_iters = len(examples), \r\n",
        "            num_desired_updates = 15\r\n",
        "        )\r\n",
        "\r\n",
        "print('Processing {:,} examples...'.format(len(examples)))\r\n",
        "\r\n",
        "# For each of the training examples...\r\n",
        "for (ex_num, ex) in enumerate(examples):\r\n",
        "\r\n",
        "    # =====================\r\n",
        "    #   Progress Update\r\n",
        "    # =====================\r\n",
        "\r\n",
        "    # Progress update every, e.g., 10k samples.\r\n",
        "    if (ex_num % update_interval) == 0 and not (ex_num == 0):\r\n",
        "\r\n",
        "        # Calculate elapsed time and format it.\r\n",
        "        elapsed = format_time(time.time() - t0)\r\n",
        "        \r\n",
        "        # Calculate the time remaining based on our progress.\r\n",
        "        ex_per_sec = (time.time() - t0) / ex_num\r\n",
        "        remaining_sec = ex_per_sec * (len(examples) - ex_num)\r\n",
        "        remaining = format_time(remaining_sec)\r\n",
        "\r\n",
        "        # Report progress.\r\n",
        "        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\r\n",
        "\r\n",
        "    # To store the start and end indeces of the three possible answers.\r\n",
        "    start_options = []\r\n",
        "    end_options = []\r\n",
        "\r\n",
        "    # Flag to indicate whether we've saved the encoded form of the input yet.\r\n",
        "    # We'll tokenize the input three times, but only need to store it once!\r\n",
        "    encoded_stored = False\r\n",
        "\r\n",
        "    # For each of the three possible answers...\r\n",
        "    for answer in ex['answers']:\r\n",
        "\r\n",
        "        # =============================\r\n",
        "        #     Add Sentinel String\r\n",
        "        # =============================\r\n",
        "        # To help us determine which of the BERT tokens correspond to the answer,\r\n",
        "        # we'll replace the answer with, e.g., \"[MASK] [MASK] [MASK]\" (based on \r\n",
        "        # the number of tokens in the answer).\r\n",
        "\r\n",
        "        # Tokenize the answer--it may be broken into multiple words and/or subwords.\r\n",
        "        answer_tokens = tokenizer.tokenize(answer['text'])\r\n",
        "\r\n",
        "        # Create our sentinel string, e.g., \"[MASK] [MASK] [MASK]\"\r\n",
        "        sentinel_str = ' '.join(['[MASK]']*len(answer_tokens))\r\n",
        "\r\n",
        "        # Within the \"context\" string, replace the answer with our sentinel.\r\n",
        "        # Python doesn't appear to have a built-in function for replacing a \r\n",
        "        # substring *starting at a specific index*, so we'll implement it in a \r\n",
        "        # more manual way.\r\n",
        "\r\n",
        "        # Locate the exact start and end of the answer text within the \"context\"\r\n",
        "        # string. The dataset gives us this information because the answer text\r\n",
        "        # may occur more than once in the context!\r\n",
        "        start_char_i = answer['answer_start']\r\n",
        "        end_char_i = start_char_i + len(answer['text'])\r\n",
        "\r\n",
        "        # To make the replacement, we use slicing and string concatenation.\r\n",
        "        context_w_sentinel = ex['context_text'][:start_char_i] + \\\r\n",
        "                            sentinel_str + \\\r\n",
        "                            ex['context_text'][end_char_i:]\r\n",
        "\r\n",
        "        # =============================\r\n",
        "        #      Tokenize & Encode\r\n",
        "        # =============================\r\n",
        "        # Combine the question and the context strings and encode them.\r\n",
        "        input_ids = tokenizer.encode(\r\n",
        "            ex['question_text'], \r\n",
        "            context_w_sentinel,\r\n",
        "            add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\r\n",
        "            #max_length = max_len,       # Pad & truncate all sentences.\r\n",
        "            pad_to_max_length = False,\r\n",
        "            truncation = False,\r\n",
        "        )\r\n",
        "\r\n",
        "        # =============================\r\n",
        "        #     Locate Answer Tokens\r\n",
        "        # =============================\r\n",
        "        # Locate all of the instances of the '[MASK]' token. \r\n",
        "\r\n",
        "        # Find all indeces of the [MASK] token.\r\n",
        "        mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\r\n",
        "\r\n",
        "        # Note: You can use the alternate code below if the input_ids are in a \r\n",
        "        #       PyTorch tensor\r\n",
        "        # First, compare all of the tokens to the mask token. \r\n",
        "        #is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\r\n",
        "        # Then get the indeces of the '1's using the `nonzero` function.\r\n",
        "        #mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\r\n",
        "\r\n",
        "        # As a sanity check, make sure the number of MASK tokens we found is the\r\n",
        "        # same as the number of answer tokens. \r\n",
        "        assert(len(mask_token_indeces) == len(answer_tokens))           \r\n",
        "\r\n",
        "        # `mask_token_indeces` is the range of indeces (e.g., [68, 69, 70, 71]), \r\n",
        "        # but we really just want the start and end indeces (e.g., 68 and 71).\r\n",
        "        start_index = mask_token_indeces[0]\r\n",
        "        end_index = mask_token_indeces[-1]\r\n",
        "\r\n",
        "        # Store these indeces in our lists.\r\n",
        "        start_options.append(start_index)\r\n",
        "        end_options.append(end_index)\r\n",
        "    \r\n",
        "    # Store the start and end indeces of the three possible correct answers.\r\n",
        "    start_positions.append(start_options)\r\n",
        "    end_positions.append(end_options)\r\n",
        "    \r\n",
        "    # Continue looping through all of the test samples.\r\n",
        "\r\n",
        "# =========================\r\n",
        "#        Wrap-Up\r\n",
        "# =========================\r\n",
        "\r\n",
        "print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 10,570 examples...\n",
            "  Example   1,000  of   10,570.    Elapsed: 0:00:06. Remaining: 0:00:55\n",
            "  Example   2,000  of   10,570.    Elapsed: 0:00:11. Remaining: 0:00:47\n",
            "  Example   3,000  of   10,570.    Elapsed: 0:00:17. Remaining: 0:00:42\n",
            "  Example   4,000  of   10,570.    Elapsed: 0:00:25. Remaining: 0:00:41\n",
            "  Example   5,000  of   10,570.    Elapsed: 0:00:34. Remaining: 0:00:38\n",
            "  Example   6,000  of   10,570.    Elapsed: 0:00:41. Remaining: 0:00:31\n",
            "  Example   7,000  of   10,570.    Elapsed: 0:00:49. Remaining: 0:00:25\n",
            "  Example   8,000  of   10,570.    Elapsed: 0:00:56. Remaining: 0:00:18\n",
            "  Example   9,000  of   10,570.    Elapsed: 0:01:03. Remaining: 0:00:11\n",
            "  Example  10,000  of   10,570.    Elapsed: 0:01:10. Remaining: 0:00:04\n",
            "DONE.  Tokenization took 0:01:16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6XYtDaaAgUB",
        "outputId": "c32e095d-defb-48d3-b56b-0f50bb014c00"
      },
      "source": [
        "num_impossible = 0\r\n",
        "num_clipped = 0\r\n",
        "\r\n",
        "# For each of the test samples...\r\n",
        "for (start_options, end_options) in zip(start_positions, end_positions):\r\n",
        "\r\n",
        "    is_possible = False\r\n",
        "\r\n",
        "    # For each of the three options...\r\n",
        "    for i in range(0, len(start_options)):\r\n",
        "        \r\n",
        "        # If at least one of the possible answers is captured, then this test \r\n",
        "        # sample is possible.\r\n",
        "        if (start_options[i] < 384) and (end_options[i] < 384):\r\n",
        "            is_possible = True\r\n",
        "        \r\n",
        "        # Tally the number of answers (across all test samples) for which\r\n",
        "        # the answer is partially or fully clipped by our truncation.\r\n",
        "        if (start_options[i] > 384) or (end_options[i] > 384):\r\n",
        "            num_clipped += 1\r\n",
        "\r\n",
        "    # Tally the number with no available answers.\r\n",
        "    if not is_possible:\r\n",
        "        num_impossible += 1\r\n",
        "\r\n",
        "print('')\r\n",
        "\r\n",
        "print('Samples w/ all answers clipped: {:,} of {:,} ({:.2%})'.format(num_impossible, len(examples), float(num_impossible) / float(len(examples))))\r\n",
        "\r\n",
        "addtl_clipped = num_clipped - (num_impossible * 3)\r\n",
        "total_answers = len(examples) * 3\r\n",
        "print('\\n    Additional clipped answers: {:,} of {:,}'.format(addtl_clipped, total_answers))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Samples w/ all answers clipped: 31 of 10,570 (0.29%)\n",
            "\n",
            "    Additional clipped answers: 19 of 31,710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7VGqILMAjr2",
        "outputId": "378e439b-f199-4dfa-a9f8-db996d2c24d3"
      },
      "source": [
        "import time\r\n",
        "import torch\r\n",
        "\r\n",
        "# Track the time. Tokenizing all training examples takes around 3 minutes.\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "# Lists to store the encoded samples.\r\n",
        "all_input_ids = []\r\n",
        "attention_masks = []\r\n",
        "segment_ids = [] \r\n",
        "\r\n",
        "# Pick an interval on which to print progress updates.\r\n",
        "update_interval = good_update_interval(\r\n",
        "            total_iters = len(examples), \r\n",
        "            num_desired_updates = 15\r\n",
        "        )\r\n",
        "\r\n",
        "print('Tokenizing {:,} examples...'.format(len(examples)))\r\n",
        "\r\n",
        "# For each of the training examples...\r\n",
        "for (ex_num, ex) in enumerate(examples):\r\n",
        "\r\n",
        "    # =====================\r\n",
        "    #   Progress Update\r\n",
        "    # =====================\r\n",
        "\r\n",
        "    # Progress update every, e.g., 10k samples.\r\n",
        "    if (ex_num % update_interval) == 0 and not (ex_num == 0):\r\n",
        "\r\n",
        "        # Calculate elapsed time and format it.\r\n",
        "        elapsed = format_time(time.time() - t0)\r\n",
        "        \r\n",
        "        # Calculate the time remaining based on our progress.\r\n",
        "        ex_per_sec = (time.time() - t0) / ex_num\r\n",
        "        remaining_sec = ex_per_sec * (len(examples) - ex_num)\r\n",
        "        remaining = format_time(remaining_sec)\r\n",
        "\r\n",
        "        # Report progress.\r\n",
        "        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\r\n",
        "\r\n",
        "    # =============================\r\n",
        "    #      Tokenize & Encode\r\n",
        "    # =============================\r\n",
        "    # Combine the question and the context strings, and tokenize them all \r\n",
        "    # together.\r\n",
        "    # `encode_plus` will:    \r\n",
        "    #   (1) Tokenize the sentence.\r\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "    #   (3) Place an `[SEP]` token between the question and reference text, and \r\n",
        "    #       and at the end of the reference text.\r\n",
        "    #   (4) Map tokens to their IDs (\"encode\" the text)\r\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "    #   (6) Create attention masks for [PAD] tokens.\r\n",
        "    #   (7) Create the list of segment IDs, indicating which tokens belong\r\n",
        "    #       to the question vs. the context.\r\n",
        "    #   (8) Casts everything as PyTorch tensors.\r\n",
        "\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "        ex['question_text'], \r\n",
        "        ex['context_text'],\r\n",
        "        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\r\n",
        "        max_length = 384,       # Pad & truncate all sentences.\r\n",
        "        pad_to_max_length = True,\r\n",
        "        truncation = True,\r\n",
        "        return_attention_mask = True, # Construct attention masks.\r\n",
        "        return_tensors = 'pt',        # Return pytorch tensors.\r\n",
        "    )\r\n",
        "\r\n",
        "    # Retrieve the encoded sequence.\r\n",
        "    input_ids = encoded_dict['input_ids']\r\n",
        "\r\n",
        "    # =============================\r\n",
        "    #     Store Encoded Sample\r\n",
        "    # =============================\r\n",
        "\r\n",
        "    # Add the encoded sentence to the list.    \r\n",
        "    all_input_ids.append(input_ids)\r\n",
        "\r\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])    \r\n",
        "\r\n",
        "    # Store the segment IDs, which indicate which tokens belong to the question\r\n",
        "    # vs. the context.\r\n",
        "    segment_ids.append(encoded_dict['token_type_ids'])\r\n",
        "\r\n",
        "    # ^^^ Continue looping through all of the test samples. ^^^\r\n",
        "\r\n",
        "# =========================\r\n",
        "#        Wrap-Up\r\n",
        "# =========================\r\n",
        "\r\n",
        "# Convert the lists of tensors into 2D tensors.\r\n",
        "all_input_ids = torch.cat(all_input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "segment_ids = torch.cat(segment_ids, dim=0)\r\n",
        "\r\n",
        "# We don't need the indeces to be tensors, since we're not doing training here.\r\n",
        "# Convert the \"labels\" (the start and end indeces) into tensors.\r\n",
        "#start_positions = torch.tensor(start_positions)\r\n",
        "#end_positions = torch.tensor(end_positions)\r\n",
        "\r\n",
        "print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing 10,570 examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Example   1,000  of   10,570.    Elapsed: 0:00:02. Remaining: 0:00:19\n",
            "  Example   2,000  of   10,570.    Elapsed: 0:00:04. Remaining: 0:00:16\n",
            "  Example   3,000  of   10,570.    Elapsed: 0:00:06. Remaining: 0:00:15\n",
            "  Example   4,000  of   10,570.    Elapsed: 0:00:08. Remaining: 0:00:13\n",
            "  Example   5,000  of   10,570.    Elapsed: 0:00:11. Remaining: 0:00:12\n",
            "  Example   6,000  of   10,570.    Elapsed: 0:00:13. Remaining: 0:00:10\n",
            "  Example   7,000  of   10,570.    Elapsed: 0:00:16. Remaining: 0:00:08\n",
            "  Example   8,000  of   10,570.    Elapsed: 0:00:18. Remaining: 0:00:06\n",
            "  Example   9,000  of   10,570.    Elapsed: 0:00:20. Remaining: 0:00:04\n",
            "  Example  10,000  of   10,570.    Elapsed: 0:00:22. Remaining: 0:00:01\n",
            "DONE.  Tokenization took 0:00:24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVbzU4SAmXG",
        "outputId": "cea2d2b9-67dd-40ec-d119-23d542b87898"
      },
      "source": [
        "import time\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Prediction on test set\r\n",
        "\r\n",
        "# Put model in evaluation mode\r\n",
        "model.eval()\r\n",
        "\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "# Tracking variables \r\n",
        "pred_start = []\r\n",
        "pred_end = []\r\n",
        "\r\n",
        "# Get the total number of test samples (not answers).\r\n",
        "num_test_samples = all_input_ids.shape[0]\r\n",
        "\r\n",
        "# We'll batch the samples to speed up processing. \r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "num_batches = int(np.ceil(num_test_samples / batch_size))\r\n",
        "\r\n",
        "print('Evaluating on {:,} test batches...'.format(num_batches))\r\n",
        "\r\n",
        "batch_num = 0\r\n",
        "\r\n",
        "# Predict \r\n",
        "for start_i in range(0, num_test_samples, batch_size):\r\n",
        "    \r\n",
        "    # Report progress.\r\n",
        "    if ((batch_num % 50) == 0) and not (batch_num == 0):\r\n",
        "\r\n",
        "        # Calculate elapsed time and format it.\r\n",
        "        elapsed = format_time(time.time() - t0)\r\n",
        "        \r\n",
        "        # Calculate the time remaining based on our progress.\r\n",
        "        batches_per_sec = (time.time() - t0) / batch_num\r\n",
        "        remaining_sec = batches_per_sec * (num_batches - batch_num)\r\n",
        "        remaining = format_time(remaining_sec)\r\n",
        "\r\n",
        "        # Report progress.\r\n",
        "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(batch_num, num_batches, elapsed, remaining))\r\n",
        "\r\n",
        "    # Calculate the ending index for this batch.\r\n",
        "    # `end_i` is equal to the index of the last sample in the batch, +1.\r\n",
        "    end_i = min(start_i + batch_size, num_test_samples)\r\n",
        "\r\n",
        "    # Select our batch inputs (`b` stands for batch here).\r\n",
        "    b_input_ids = all_input_ids[start_i:end_i, :]\r\n",
        "    b_attn_masks = attention_masks[start_i:end_i, :]\r\n",
        "    b_seg_ids = segment_ids[start_i:end_i, :]   \r\n",
        "\r\n",
        "    # Copy these to the GPU.\r\n",
        "    b_input_ids = b_input_ids.to(device)\r\n",
        "    b_attn_masks = b_attn_masks.to(device)\r\n",
        "    b_seg_ids = b_seg_ids.to(device)\r\n",
        "    \r\n",
        "    # Telling the model not to compute or store the compute graph, saving memory \r\n",
        "    # and speeding up prediction\r\n",
        "    with torch.no_grad():\r\n",
        "        \r\n",
        "        # Forward pass, calculate logit predictions\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                        attention_mask=b_attn_masks,\r\n",
        "                        token_type_ids=b_seg_ids)\r\n",
        "                        \r\n",
        "    start_logits = outputs[0]\r\n",
        "    end_logits = outputs[1]\r\n",
        "    # Move logits and labels to CPU\r\n",
        "    start_logits = start_logits.detach().cpu().numpy()\r\n",
        "    end_logits = end_logits.detach().cpu().numpy()\r\n",
        "    \r\n",
        "    # Find the tokens with the highest `start` and `end` scores.\r\n",
        "    answer_start = np.argmax(start_logits, axis=1)\r\n",
        "    answer_end = np.argmax(end_logits, axis=1)\r\n",
        "\r\n",
        "    # Store predictions and true labels\r\n",
        "    pred_start.append(answer_start)\r\n",
        "    pred_end.append(answer_end)\r\n",
        "\r\n",
        "    batch_num += 1\r\n",
        "\r\n",
        "    # ^^^ Continue looping through the batches. ^^^\r\n",
        "\r\n",
        "# Combine the results across the batches.\r\n",
        "pred_start = np.concatenate(pred_start, axis=0)\r\n",
        "pred_end = np.concatenate(pred_end, axis=0)\r\n",
        "\r\n",
        "print('    DONE.')\r\n",
        "\r\n",
        "print('\\nEvaluation took {:.0f} seconds.'.format(time.time() - t0))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating on 661 test batches...\n",
            "  Batch      50  of      661.    Elapsed: 0:00:22. Remaining: 0:04:31\n",
            "  Batch     100  of      661.    Elapsed: 0:00:45. Remaining: 0:04:15\n",
            "  Batch     150  of      661.    Elapsed: 0:01:07. Remaining: 0:03:49\n",
            "  Batch     200  of      661.    Elapsed: 0:01:29. Remaining: 0:03:25\n",
            "  Batch     250  of      661.    Elapsed: 0:01:51. Remaining: 0:03:03\n",
            "  Batch     300  of      661.    Elapsed: 0:02:14. Remaining: 0:02:41\n",
            "  Batch     350  of      661.    Elapsed: 0:02:36. Remaining: 0:02:18\n",
            "  Batch     400  of      661.    Elapsed: 0:02:58. Remaining: 0:01:56\n",
            "  Batch     450  of      661.    Elapsed: 0:03:20. Remaining: 0:01:34\n",
            "  Batch     500  of      661.    Elapsed: 0:03:42. Remaining: 0:01:12\n",
            "  Batch     550  of      661.    Elapsed: 0:04:05. Remaining: 0:00:49\n",
            "  Batch     600  of      661.    Elapsed: 0:04:27. Remaining: 0:00:27\n",
            "  Batch     650  of      661.    Elapsed: 0:04:49. Remaining: 0:00:05\n",
            "    DONE.\n",
            "\n",
            "Evaluation took 294 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmVyg3FdAr_e",
        "outputId": "ec24f196-873e-4e59-9707-f5201673be93"
      },
      "source": [
        "\r\n",
        "total_correct = 0\r\n",
        "\r\n",
        "# For each test sample...\r\n",
        "for i in range(0, len(pred_start)):\r\n",
        "\r\n",
        "    match_options = []\r\n",
        "\r\n",
        "    # For each of the three possible answers...\r\n",
        "    for j in range (0, len(start_positions[i])):\r\n",
        "    \r\n",
        "        matches = 0\r\n",
        "\r\n",
        "        # Add a point if the start indeces match.\r\n",
        "        if pred_start[i] == start_positions[i][j]:\r\n",
        "            matches += 1\r\n",
        "\r\n",
        "        # Add a point if the end indeces match.\r\n",
        "        if pred_end[i] == end_positions[i][j]:\r\n",
        "            matches += 1\r\n",
        "\r\n",
        "        # Store the total.\r\n",
        "        match_options.append(matches)\r\n",
        "\r\n",
        "    # Between the three possible answers, pick the one with the highest \"score\".\r\n",
        "    total_correct += (max(match_options))\r\n",
        "\r\n",
        "    # ^^^ Continue looping through test samples ^^^\r\n",
        "\r\n",
        "total_indeces = len(pred_start) + len(pred_end)\r\n",
        "\r\n",
        "print('Correctly predicted indeces: {:,} of {:,} ({:.2%})'.format(\r\n",
        "    total_correct,\r\n",
        "    total_indeces,\r\n",
        "    float(total_correct) / float(total_indeces)\r\n",
        "))\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correctly predicted indeces: 17,729 of 21,140 (83.86%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDwTWBLQAuku",
        "outputId": "cec34e4a-c655-4ef8-ea6c-55fea1838563"
      },
      "source": [
        "# The final F1 score for each sample.\r\n",
        "f1s = []\r\n",
        "\r\n",
        "# For each test sample...\r\n",
        "for i in range(0, len(pred_start)):\r\n",
        "\r\n",
        "    # Expand the start and end indeces into sequences of indeces stored as sets.\r\n",
        "    # For example, if pred_start = 137 and pred_end = 140, then\r\n",
        "    #   pred_span = {137, 138, 139, 140}\r\n",
        "    pred_span = set(range(pred_start[i], pred_end[i] + 1))\r\n",
        "\r\n",
        "\r\n",
        "    f1_options = []\r\n",
        "\r\n",
        "    # For each of the three possible answers...\r\n",
        "    for j in range (0, len(start_positions[i])):\r\n",
        "    \r\n",
        "        # Expand this answer into a range, as above.\r\n",
        "        true_span = set(range(start_positions[i][j], end_positions[i][j] + 1))    \r\n",
        "\r\n",
        "        # Use the `intersection` function from Python `set` to get the set of \r\n",
        "        # indeces occurring in both spans. Take the length of this resulting set\r\n",
        "        # as the number of overlapping indeces between the two spans.\r\n",
        "        num_same = len(pred_span.intersection(true_span))    \r\n",
        "\r\n",
        "        # If there's no overlap, then the F1 score is 0 for this sample.\r\n",
        "        if num_same == 0:\r\n",
        "            f1_options.append(0)\r\n",
        "            continue\r\n",
        "\r\n",
        "        # Precision - How many tokens overlap relative to the total number of tokens\r\n",
        "        #             in the predicted span? If the model predicts too large of a \r\n",
        "        #             span, it has bad precision.      \r\n",
        "        precision = float(num_same) / float(len(pred_span))\r\n",
        "    \r\n",
        "        # Recall - How many of the correct tokens made it into the predicted span?\r\n",
        "        #          A model could have perfect recall if it just predicted the entire\r\n",
        "        #          paragraph as the answer :).    \r\n",
        "        recall = float(num_same) / float(len(true_span))\r\n",
        "\r\n",
        "        # F1 - Does the model have both good precision and good recall?\r\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\r\n",
        "\r\n",
        "        # Store the score.\r\n",
        "        f1_options.append(f1)\r\n",
        "\r\n",
        "        # ^^^ Continue looping through possible answers ^^^\r\n",
        "\r\n",
        "    # Take the highest of the three F1 scores as our score for this sample.\r\n",
        "    f1s.append(max(f1_options))\r\n",
        "\r\n",
        "    # ^^^ Continue looping through test samples ^^^\r\n",
        "\r\n",
        "\r\n",
        "print('Average F1 Score: {:.3f}'.format(np.mean(f1s)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average F1 Score: 0.862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-S0VkGogi7Y",
        "outputId": "ea2a7dcc-898e-450d-c6a6-06ca3b070867"
      },
      "source": [
        "output_dir = './pretrained_model/'\r\n",
        "\r\n",
        "# Create output directory if needed\r\n",
        "if not os.path.exists(output_dir):\r\n",
        "    os.makedirs(output_dir)\r\n",
        "\r\n",
        "print(\"Saving model to %s\" % output_dir)\r\n",
        "\r\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\r\n",
        "# They can then be reloaded using `from_pretrained()`\r\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\r\n",
        "model_to_save.save_pretrained(output_dir)\r\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./pretrained_model/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./pretrained_model/tokenizer_config.json',\n",
              " './pretrained_model/special_tokens_map.json',\n",
              " './pretrained_model/vocab.txt',\n",
              " './pretrained_model/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8s7Id8Xgv05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}